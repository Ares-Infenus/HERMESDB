{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de Archivos SQL\n",
    "\n",
    "En esta sección, se importa y organiza la información contenida en archivos SQL, separándolos por carpetas o claves para su uso posterior. El objetivo es asegurar que la estructura de los datos sea consistente y adecuada para su procesamiento posterior.\n",
    "\n",
    "El proceso incluye las siguientes verificaciones:\n",
    "\n",
    "- **Validación de archivos**: Se revisa si todos los archivos necesarios están presentes, asegurando que no falten archivos importantes.\n",
    "- **Detección de duplicados**: Se comprueba si existen registros duplicados en los datos importados.\n",
    "- **Verificación de la consistencia de las dimensiones**: Se asegura que todos los archivos tengan el mismo número de filas y columnas, para evitar errores de incompatibilidad.\n",
    "- **Estructura de los datos**: Se valida que cada DataFrame importado tenga la siguiente estructura:\n",
    "  - **Data time**\n",
    "  - **Close**\n",
    "  - **Open**\n",
    "  - **High**\n",
    "  - **Low**\n",
    "  - **Volume**\n",
    "\n",
    "El objetivo es garantizar que todos los datos estén bien organizados, sin errores de duplicidad ni inconsistencias, para su posterior análisis y procesamiento en el sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificación superada: Todos los criterios cumplen los requisitos establecidos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing DataFrames: 100%|██████████| 12/12 [00:14<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.import_modules.security_import.DuplicateChecker object at 0x0000024E8402DC40>\n"
     ]
    }
   ],
   "source": [
    "from modules.import_modules.import_m import cargar_dataframes, agregar_extension_csv #Dataframe de importacion, identificador de dataframes.\n",
    "from modules.import_modules.security_import import verificar_diccionario,DuplicateChecker #Verificacion de los datos importados\n",
    "from modules.import_modules.print_situation_import import validar_diccionario # Reporte de los check o verificaciones de los datos importados\n",
    "\n",
    "\n",
    "\n",
    "#importando en un diccionario:\n",
    "carpeta_base = 'C:\\\\Users\\\\spinz\\\\Documents\\\\Portafolio Oficial\\\\HERMESDB\\\\data\\\\raw' # Directorio de los datos sin procesar\n",
    "diccionario_datos = cargar_dataframes(carpeta_base) #Diccionario con los dataframes de importacion\n",
    "diccionario_datos = agregar_extension_csv(diccionario_datos) #Agregando la extension csv a los dataframes importados\n",
    "\n",
    "#Verificacion de los datos importados\n",
    "verificacion_1 = verificar_diccionario(diccionario_datos) # Verifica la estructura y validez de los archivos CSV en un diccionario de datos.\n",
    "verificacion_1 = verificacion_1['Resumen'] # Resumen de la verificación 1\n",
    "resultado = validar_diccionario(verificacion_1)\n",
    "print(resultado)\n",
    "import nest_asyncio #Libreria necesaria para que funcione el asyncio en jupyter\n",
    "nest_asyncio.apply() # Aplicando la libreria nest_asyncio\n",
    "if __name__ == '__main__':\n",
    "    # Crear un diccionario con DataFrames de ejemplo\n",
    "    # Instanciar la clase y ejecutar la detección de duplicados\n",
    "    verificacion_2 = DuplicateChecker(diccionario_datos)\n",
    "    verificacion_2.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza y Adecuación de DataFrames\n",
    "\n",
    "## Descripción\n",
    "\n",
    "En el proceso de análisis de datos, es crucial trabajar con información limpia, estructurada y consistente. Esta etapa tiene como objetivo garantizar que los DataFrames almacenados en un diccionario cumplan con los requisitos técnicos para un análisis avanzado y confiable. Se eliminan valores no válidos y se estandarizan formatos para evitar errores en etapas posteriores.\n",
    "\n",
    "### Eliminación de Valores No Válidos\n",
    "\n",
    "Se identifican y eliminan valores que pueden comprometer la calidad de los datos, tales como:\n",
    "\n",
    "- **Valores nulos (`NaN`)**: Indicadores de datos faltantes que podrían distorsionar cálculos.\n",
    "- **Valores infinitos (`Inf`)**: Resultados de operaciones inválidas que deben ser tratados como errores.\n",
    "- **Filas inconsistentes**: Aquellas que contienen información no procesable o incompleta.\n",
    "\n",
    "Esto asegura que cada fila de datos sea válida y útil para el análisis.\n",
    "\n",
    "### Adecuación de Formatos de Columna\n",
    "\n",
    "Para garantizar la coherencia en los cálculos, se verifica y ajusta el formato de todas las columnas:\n",
    "\n",
    "- La columna `datetime` debe estar presente y ser convertida al formato **datetime64**, lo que permite un manejo eficiente de datos temporales.\n",
    "- Las columnas restantes deben estar en formato **float64**, asegurando precisión en los valores numéricos.\n",
    "\n",
    "Cualquier fila que no cumpla con estos criterios será eliminada.\n",
    "\n",
    "### Ordenamiento y Reindexación\n",
    "\n",
    "Una vez procesados los datos, la columna `datetime` se utiliza para ordenar cronológicamente las filas de cada DataFrame, lo que facilita la interpretación y el análisis temporal. Finalmente, los DataFrames se reindexan para garantizar un índice limpio y continuo.\n",
    "\n",
    "### Resultado Esperado\n",
    "\n",
    "El resultado de este proceso es un conjunto de DataFrames que:\n",
    "\n",
    "1. **Carecen de valores no válidos**: Sin `NaN`, `Inf` ni inconsistencias.\n",
    "2. **Tienen columnas con formatos correctos**: `datetime` en **datetime64** y demás columnas en **float64**.\n",
    "3. **Están estructurados y listos para análisis**: Ordenados cronológicamente y con un índice consistente.\n",
    "\n",
    "Este paso es fundamental para garantizar que las operaciones posteriores, como visualización, modelado o análisis estadístico, se realicen de manera eficiente y precisa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<modules.import_modules.security_import.DuplicateChecker object at 0x0000024EAEDCD070>\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
