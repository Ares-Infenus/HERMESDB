{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de Archivos SQL\n",
    "\n",
    "En esta sección, se importa y organiza la información contenida en archivos SQL, separándolos por carpetas o claves para su uso posterior. El objetivo es asegurar que la estructura de los datos sea consistente y adecuada para su procesamiento posterior.\n",
    "\n",
    "El proceso incluye las siguientes verificaciones:\n",
    "\n",
    "- **Validación de archivos**: Se revisa si todos los archivos necesarios están presentes, asegurando que no falten archivos importantes.\n",
    "- **Detección de duplicados**: Se comprueba si existen registros duplicados en los datos importados.\n",
    "- **Verificación de la consistencia de las dimensiones**: Se asegura que todos los archivos tengan el mismo número de filas y columnas, para evitar errores de incompatibilidad.\n",
    "- **Estructura de los datos**: Se valida que cada DataFrame importado tenga la siguiente estructura:\n",
    "  - **Data time**\n",
    "  - **Close**\n",
    "  - **Open**\n",
    "  - **High**\n",
    "  - **Low**\n",
    "  - **Volume**\n",
    "\n",
    "El objetivo es garantizar que todos los datos estén bien organizados, sin errores de duplicidad ni inconsistencias, para su posterior análisis y procesamiento en el sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificación superada: Todos los criterios cumplen los requisitos establecidos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing DataFrames: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from modules.import_modules.import_m import cargar_dataframes, agregar_extension_csv #Dataframe de importacion, identificador de dataframes.\n",
    "from modules.import_modules.security_import import verificar_diccionario,DuplicateChecker #Verificacion de los datos importados\n",
    "from modules.import_modules.print_situation_import import validar_diccionario # Reporte de los check o verificaciones de los datos importados\n",
    "\n",
    "\n",
    "\n",
    "#importando en un diccionario:\n",
    "carpeta_base = 'C:\\\\Users\\\\spinz\\\\Documents\\\\Portafolio Oficial\\\\HERMESDB\\\\data\\\\raw' # Directorio de los datos sin procesar\n",
    "diccionario_datos = cargar_dataframes(carpeta_base) #Diccionario con los dataframes de importacion\n",
    "diccionario_datos = agregar_extension_csv(diccionario_datos) #Agregando la extension csv a los dataframes importados\n",
    "\n",
    "#Verificacion de los datos importados\n",
    "verificacion_1 = verificar_diccionario(diccionario_datos) # Verifica la estructura y validez de los archivos CSV en un diccionario de datos.\n",
    "verificacion_1 = verificacion_1['Resumen'] # Resumen de la verificación 1\n",
    "resultado = validar_diccionario(verificacion_1)\n",
    "print(resultado)\n",
    "import nest_asyncio #Libreria necesaria para que funcione el asyncio en jupyter debido a que esto usando la cpu\n",
    "nest_asyncio.apply() # Aplicando la libreria nest_asyncio\n",
    "if __name__ == '__main__':\n",
    "    # Crear un diccionario con DataFrames de ejemplo\n",
    "    # Instanciar la clase y ejecutar la detección de duplicados\n",
    "    verificacion_2 = DuplicateChecker(diccionario_datos)\n",
    "    verificacion_2.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza y Adecuación de DataFrames\n",
    "\n",
    "## Descripción\n",
    "\n",
    "En el proceso de análisis de datos, es crucial trabajar con información limpia, estructurada y consistente. Esta etapa tiene como objetivo garantizar que los DataFrames almacenados en un diccionario cumplan con los requisitos técnicos para un análisis avanzado y confiable. Se eliminan valores no válidos y se estandarizan formatos para evitar errores en etapas posteriores.\n",
    "\n",
    "### Eliminación de Valores No Válidos\n",
    "\n",
    "Se identifican y eliminan valores que pueden comprometer la calidad de los datos, tales como:\n",
    "\n",
    "- **Valores nulos (`NaN`)**: Indicadores de datos faltantes que podrían distorsionar cálculos.\n",
    "- **Valores infinitos (`Inf`)**: Resultados de operaciones inválidas que deben ser tratados como errores.\n",
    "- **Filas inconsistentes**: Aquellas que contienen información no procesable o incompleta.\n",
    "\n",
    "Esto asegura que cada fila de datos sea válida y útil para el análisis.\n",
    "\n",
    "### Adecuación de Formatos de Columna\n",
    "\n",
    "Para garantizar la coherencia en los cálculos, se verifica y ajusta el formato de todas las columnas:\n",
    "\n",
    "- La columna `datetime` debe estar presente y ser convertida al formato **datetime64**, lo que permite un manejo eficiente de datos temporales.\n",
    "- Las columnas restantes deben estar en formato **float64**, asegurando precisión en los valores numéricos.\n",
    "\n",
    "Cualquier fila que no cumpla con estos criterios será eliminada.\n",
    "\n",
    "### Ordenamiento y Reindexación\n",
    "\n",
    "Una vez procesados los datos, la columna `datetime` se utiliza para ordenar cronológicamente las filas de cada DataFrame, lo que facilita la interpretación y el análisis temporal. Finalmente, los DataFrames se reindexan para garantizar un índice limpio y continuo.\n",
    "\n",
    "### Resultado Esperado\n",
    "\n",
    "El resultado de este proceso es un conjunto de DataFrames que:\n",
    "\n",
    "1. **Carecen de valores no válidos**: Sin `NaN`, `Inf` ni inconsistencias.\n",
    "2. **Tienen columnas con formatos correctos**: `datetime` en **datetime64** y demás columnas en **float64**.\n",
    "3. **Están estructurados y listos para análisis**: Ordenados cronológicamente y con un índice consistente.\n",
    "\n",
    "Este paso es fundamental para garantizar que las operaciones posteriores, como visualización, modelado o análisis estadístico, se realicen de manera eficiente y precisa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crypto': {'BTCUSD': {'1D': {'BTCUSD_1D_ASK.csv':                            Price\n",
      "Date                            \n",
      "2020-01-02 00:00:00+00:00   7100\n",
      "2020-01-01 00:00:00+00:00   7000}}}}\n",
      "Carpeta vacía o archivo no compatible, revise documentación: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 29.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files processed: 50\n",
      "Total rows removed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from modules.Limpieza_y_normalizacion.Localtime_normalize import process_hierarchical_dict #Normalizacion de la columna de tiempo\n",
    "from modules.Limpieza_y_normalizacion.DataRefiner import DataCleaner #Limpieza de los datos\n",
    "#Para sebastian del mañana recuera que falta el verificador de columnas \n",
    "\n",
    "#y no se te olvide la exportacion de ello. en la carpeta de preprocesados\n",
    "# tambien recuerda como solucioanr el problema de la normalizacion de la columna de tiempo.\n",
    "#Normalizacion de la columna de tiempo \n",
    "diccionario_datos = process_hierarchical_dict(diccionario_datos) #Normalizando la columna de tiempo\n",
    "\n",
    "# Llamando a la clase DataCleaner\n",
    "cleaner = DataCleaner(max_missing_allowed=2)\n",
    "# Llamando al método run para limpiar los datos\n",
    "cleaned_data = cleaner.clean_data(diccionario_datos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Open      High       Low     Close  \\\n",
      "Date                                                                \n",
      "2024-12-14 22:00:00+00:00  2649.325  2649.325  2649.325  2649.325   \n",
      "2024-12-13 22:00:00+00:00  2649.325  2649.325  2649.325  2649.325   \n",
      "2024-12-12 22:00:00+00:00  2681.702  2693.155  2646.472  2649.325   \n",
      "2024-12-11 22:00:00+00:00  2718.675  2726.526  2675.352  2681.986   \n",
      "2024-12-10 22:00:00+00:00  2695.102  2721.635  2676.145  2718.995   \n",
      "...                             ...       ...       ...       ...   \n",
      "2004-01-04 22:00:00+00:00   414.682   424.411   414.563   423.339   \n",
      "2004-01-03 22:00:00+00:00   415.805   415.805   415.805   415.805   \n",
      "2004-01-02 22:00:00+00:00   415.805   415.805   415.805   415.805   \n",
      "2004-01-01 22:00:00+00:00   415.297   416.505   414.353   415.805   \n",
      "2003-12-31 22:00:00+00:00   415.430   415.805   414.695   415.153   \n",
      "\n",
      "                                 Volume  \n",
      "Date                                     \n",
      "2024-12-14 22:00:00+00:00  0.000000e+00  \n",
      "2024-12-13 22:00:00+00:00  0.000000e+00  \n",
      "2024-12-12 22:00:00+00:00  7.552898e+07  \n",
      "2024-12-11 22:00:00+00:00  9.201206e+07  \n",
      "2024-12-10 22:00:00+00:00  1.030071e+08  \n",
      "...                                 ...  \n",
      "2004-01-04 22:00:00+00:00  1.088688e+07  \n",
      "2004-01-03 22:00:00+00:00  0.000000e+00  \n",
      "2004-01-02 22:00:00+00:00  0.000000e+00  \n",
      "2004-01-01 22:00:00+00:00  4.411670e+06  \n",
      "2003-12-31 22:00:00+00:00  6.249040e+05  \n",
      "\n",
      "[7655 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(diccionario_datos['metals']['XAUUSD']['1D']['XAUUSD_1D_ASK.csv'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
